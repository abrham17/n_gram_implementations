{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "V28",
      "authorship_tag": "ABX9TyNpew1lmnGYQqL0+WYszhvA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/abrham17/n_gram_implementations/blob/main/n_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sJB_TGc_Kb_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **N-gram Implementation**\n"
      ],
      "metadata": {
        "id": "kRXOX2dxygR1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can access this corpus using this link:\n",
        "link.https://drive.google.com/file/d/1WnuCIeZglWU0uty8-uMOBwOWQM-zAHBJ/view?usp=shari\n",
        "ng\n",
        "The corpus may require a good size of RAM to load; hence you can make use of\n",
        "Google Collab, Kaggle or more if your device is incapable. and try to avoid using variables to\n",
        "store the result of the each part so that you can use it later since it is a big data it will take a lot\n",
        "of your memory and you will run out of it.\n"
      ],
      "metadata": {
        "id": "1cc0ltX5yY9_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NOTE: IF YOU WANT TO WORK ON ANOTHER CORPUS THEN YOU ARE WELCOME TO\n",
        "DO SO. IF YOU DO MAKE SURE TO CHANGE UP SOME OF THE QUESTIONS TO\n",
        "ACCOMMODATE YOUR DATASET EG. 1.4 AND 1.6"
      ],
      "metadata": {
        "id": "SYp3Poi1y4k0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "yhU-TdgJeMUb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.1 Create n-grams for n=1, 2, 3, 4. You can show sample prints.**"
      ],
      "metadata": {
        "id": "L_-dbBpby9EA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating 1, 2, 3 , 4 grams from dataset.txt. If there is a stop word (as specified in question 4) if will not include in the grams.\n"
      ],
      "metadata": {
        "id": "mad9RfqizHl5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import re\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "def corpus_ngram(n , stop_word=None):\n",
        "     try:\n",
        "         output_file = \"/content/drive/MyDrive/dataset.txt\"\n",
        "         with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "             text = f.read()\n",
        "         words = re.split(r'(\\W+)', text.strip())\n",
        "         words = [w for w in words if w.strip()]\n",
        "\n",
        "         if stop_word is not None:\n",
        "             words = [word for word in words if word not in stop_word]\n",
        "         return [tuple(words[i:i + n]) for i in range(len(words) - n + 1)]\n",
        "\n",
        "     except Exception as e:\n",
        "         print(f\"Exception: {e}\")\n",
        "uningram = corpus_ngram(1)\n",
        "bigram = corpus_ngram(2)\n",
        "trigram = corpus_ngram(3)\n",
        "qatragram = corpus_ngram(4)\n",
        "for i in range(10):\n",
        "  print(uningram[i])\n",
        "  print(bigram[i])\n",
        "  print(trigram[i])\n",
        "  print(qatragram[i])"
      ],
      "metadata": {
        "id": "093Ltqb8lpKm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "24689493-0756-4fd4-a109-14648f23cd9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.2 Calculate probabilities of n-grams and find the top 10 most likely n-grams for all n.**"
      ],
      "metadata": {
        "id": "lUtG4GJ7zqjW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "used a dictionary for simpler access if needed in another times.\n"
      ],
      "metadata": {
        "id": "2Z19WEboz2Q-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_probability(ngrams = 'unigram' , amount=10):\n",
        "    return dict(sorted(((ngram, count / len(ngrams)) for ngram, count in Counter(ngrams).items()),key=lambda item: item[1], reverse=True)[:amount])\n",
        "\n",
        "print(f\"top 10 unigram {calculate_probability(uningram)}\")"
      ],
      "metadata": {
        "id": "A-BrpFe4P_Tw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "531ab06f-3610-43f6-fec5-1845bf85e646"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top 10 unigram {(' ። ',): 0.022575064928581485, ('ነው',): 0.013514283511508452, ('፡፡ ',): 0.012004114647540564, ('፣ ',): 0.010399176365921015, ('። ',): 0.008798850242313859, ('ላይ',): 0.008744524119796006, (' ፣ ',): 0.004529730173343787, ('ውስጥ',): 0.004012913532547889, ('ወደ',): 0.003835426567427705, ('፤ ',): 0.0037990391901439897}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.3 Calculate the conditional probability of a word given the previous word using bigrams.**\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O98XdjgC0VDU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "calculating conditional probabilites in bigrams using the formula:\n",
        "\n",
        "P(B|A) = P(B and A) / P(A)\n",
        "\n",
        "for instance to calculate the conditional probability of \"ታሪካዊ\" in the bigram \"ኢትዮጵያ ታሪካዊ\"\n",
        "\n",
        "P(ታሪካዊ|ኢትዮጵያ) = P(ኢትዮጵያ ታሪካዊ)/P(ኢትዮጵያ)"
      ],
      "metadata": {
        "id": "wib5duaq0ep_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def conditional_probability( sample_grams=\"ኢትዮጵያ ታሪካዊ\"):\n",
        "    words = sample_grams.split(\" \")\n",
        "    bigram_count = Counter(bigram)\n",
        "    unigram_count = Counter(uningram)\n",
        "    if tuple(words) in bigram_count and tuple([words[0]]) in unigram_count:\n",
        "        probability = bigram_count[tuple(words)] / unigram_count[tuple([words[0]])]\n",
        "    else:\n",
        "        probability = 0\n",
        "    return probability\n",
        "print(f\"condiftional probability of biagram {'ታሪካዊ'} in {'ኢትዮጵያ ታሪካዊ'} {conditional_probability()}\")\n",
        "print(f\"condiftional probability of biagram {'ምን'} in {'ምን መሰላችሁ'} {conditional_probability('ምን መሰላችሁ')}\")"
      ],
      "metadata": {
        "id": "fhyUS49UaRn2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b147955-7364-4821-cb64-b630d6cf7a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "condiftional probability of biagram ታሪካዊ in ኢትዮጵያ ታሪካዊ 0.000302259143339086\n",
            "condiftional probability of biagram ምን in ምን መሰላችሁ 0.008140165378880303\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1O9zlNyLbEgf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.4 Remove common stopwords and recompute bigram and trigram frequencies, find\n",
        "the top 10 n-grams; n=1,2,3,4.(OPTIONAL) # FIND THE MOST COMMON AMHARIC\n",
        "STOPWORDS BY YOURSELF**"
      ],
      "metadata": {
        "id": "EYjDKPzs29dM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function ,\"recompute_bigram_trigram\", recompute the bigram and trigram again removing amharic stop words. this stop words are passed to the function corpus_ngram which has default None for stop words."
      ],
      "metadata": {
        "id": "1zfSc8nS3Df8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "amharic_stop_words = [\n",
        "    \"እኔ\", \"አንቺ\", \"አንተ\", \"እሱ\", \"እሷ\", \"እኛ\", \"እነሱ\",\n",
        "    \"ይህ\", \"ያ\", \"ይች\", \"እነዚህ\",\n",
        "    \"እና\", \"ወይም\", \"ነገር ግን\", \"እንዲሁም\",\n",
        "    \"ላይ\", \"ስለ\", \"ከ\", \"በ\", \"ማዶ\", \"ውስጥ\",\n",
        "    \"እየ\", \"ሲ\", \"የሚ\", \"ይሄ\",\n",
        "    \"ነው\", \"ናት\", \"አለ\", \"አሉ\", \"እንደ\", \"የት\", \"ማን\", \"ምን\",\n",
        "    \"እንደዚህ\", \"በዚህ\", \"ይህን\", \"ይሄን\", \"ይሆናል\",\n",
        "]\n",
        "\n",
        "unigrams_filtered = corpus_ngram(1, stop_word=amharic_stop_words)\n",
        "bigrams_filtered = corpus_ngram(2, stop_word=amharic_stop_words)\n",
        "trigrams_filtered = corpus_ngram(3, stop_word=amharic_stop_words)\n",
        "\n",
        "def recompute_bigram_trigram():\n",
        "  return calculate_probability(bigrams_filtered) , calculate_probability(trigrams_filtered)\n",
        "\n",
        "print(f\"top 10 bigram {recompute_bigram_trigram()[0]}\")\n",
        "print(f\"top 10 trigram {recompute_bigram_trigram()[1]}\")\n"
      ],
      "metadata": {
        "id": "YY9TrvQmy0xM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.5 Create word clouds for unigrams, bigrams, and trigrams before and after stop word removal. (OPTIONAL)**"
      ],
      "metadata": {
        "id": "XwGJOCPH40Fh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "\n",
        "def generate_wordcloud(ngrams, title=\"Word Cloud for amharic corpus\"):\n",
        "    ngram_freq = Counter(ngrams)\n",
        "    wordcloud = WordCloud(width=800, height=400, background_color=\"white\").generate_from_frequencies(ngram_freq)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "    plt.axis(\"off\")\n",
        "    plt.title(title, fontsize=14)\n",
        "    plt.show()\n",
        "\n",
        "unigrams = corpus_ngram(1)\n",
        "bigrams = corpus_ngram(2)\n",
        "trigrams = corpus_ngram(3)\n",
        "\n",
        "generate_wordcloud(unigrams, \"Unigram Word Cloud\")\n",
        "generate_wordcloud(bigrams, \"Bigram Word Cloud\")\n",
        "generate_wordcloud(trigrams, \"Trigram Word Cloud\")\n",
        "\n",
        "generate_wordcloud(unigrams_filtered, \"Unigram Word Cloud (After Stop Word Removal)\")\n",
        "generate_wordcloud(bigrams_filtered, \"Bigram Word Cloud (After Stop Word Removal)\")\n",
        "generate_wordcloud(trigrams_filtered, \"Trigram Word Cloud (After Stop Word Removal)\")\n"
      ],
      "metadata": {
        "id": "cM5pfLb2ChgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.6 What is the probability of the sentence. \"ኢትዮጵያ ታሪካዊ ሀገር ናት \". You can also try more sentences.**"
      ],
      "metadata": {
        "id": "lUyz13PP4zrh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to calculate the probability of word combinations, \"ኢትዮጵያ ታሪካዊ ሀገር ናት\".\n",
        "\n",
        "\n",
        "1.   count the number of words and prepare n gram based on the word count\n",
        "2. Count how many times the word combination happens in the ngram 3\n",
        "3  Divide the number of times the word combination in ngram by the total number of ngrams\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "EgmT7OQ95BV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_pro = \"ኢትዮጵያ ታሪካዊ ሀገር ናት\"\n",
        "#corpus_ngram(\"https://drive.google.com/file/d/1WnuCIeZglWU0uty8-uMOBwOWQM-zAHBJ/view?usp=sharing\", 2 , stop_words)\n",
        "def calcualte_pro_of_specfic_word(word):\n",
        "  word_count = word.split(\" \")\n",
        "  ngrams = corpus_ngram(len(word_count))\n",
        "  counts = Counter(ngrams)\n",
        "  return counts[tuple(word)] / len(ngrams)\n",
        "print(f\"word combination for 'ኢትዮጵያ ታሪካዊ ሀገር ናት' is {calcualte_pro_of_specfic_word(word_pro)}\")"
      ],
      "metadata": {
        "id": "krkg0YFSEdpB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1.7 Generate random sentences using n-grams; explain what happens as n-increases based on your output.**"
      ],
      "metadata": {
        "id": "7vF7jszf6Os_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generating a random number requires the probability of total ngrams.\n",
        "\n",
        "**steps**:\n",
        "\n",
        "\n",
        "1.   preparing ngrams based on input n\n",
        "2.   selecting words with highest probability\n",
        "3.   creating a sentence using the the highest probability consisting of n words\n",
        "\n"
      ],
      "metadata": {
        "id": "mtqZigYq6V5y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_random_sentence(n_gram = 1 , number_of_words=10):\n",
        "  ngram = corpus_ngram(n_gram)\n",
        "  words_highest_pro = calculate_probability(ngram , number_of_words)\n",
        "  return \" \".join(words_highest_pro.keys())\n",
        "print(f\" Generated 10 word combination from trigrams is {generate_random_sentence(3)}\")"
      ],
      "metadata": {
        "id": "iqNU_PB1MjPC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OPTIONAL"
      ],
      "metadata": {
        "id": "tLnYkyizFF4I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#2 Evaluate these Language Models Using Intrinsic Evaluation Metho(Perplexity)**"
      ],
      "metadata": {
        "id": "mz6ZPy78EwIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "def calculate_perplexity(sentence, n=2):\n",
        "    words = sentence.split(\" \")\n",
        "    probabilities = [conditional_probability(n, \" \".join(words[i:i+n])) for i in range(len(words)-n+1)]\n",
        "    return np.exp(-np.mean(np.log(probabilities)))\n"
      ],
      "metadata": {
        "id": "ohGS6XokYJ51"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**#3 Evaluate these Language Models Using Extrinsic Evaluation Method  \n",
        "You can make use of any task convenient to you to evaluate the n-gram models you have created.**"
      ],
      "metadata": {
        "id": "laxq2WBVFhhL"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AKXwcu82FhKw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}